{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc12ecf2",
   "metadata": {},
   "source": [
    "* Player objectives\n",
    "    * Get the AI to say a specific word\n",
    "    * Avoid taboo words\n",
    "    * Avoid responses with high loss\n",
    "* Escalation\n",
    "    * Seed the AI with thoughts ex. \"I hate you\\n\\<user input\\>\"\n",
    "        * Punishment for saying taboo words\n",
    "    * Choose responses that are classified as angry\n",
    "        * Punishment for responses with high loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea5846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6aaddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceec144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "chat_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda1c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc02207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = chat_model.to(DEVICE)\n",
    "emotion_model = emotion_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb93e2a",
   "metadata": {},
   "source": [
    "## Seeding AI thoughts ##\n",
    "We can encourage the AI to respond more aggressively by asking it to generate a response while pretending that it previously said an aggressive response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91069d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> I hate you.  It is so hot.  I am ready for it to cool</s>', '<s> I hate you.  It is so hot and humid.  I am ready for</s>', '<s> I hate you.  It is so hot.  I am ready for fall. </s>', '<s> I hate you.  It is so hot and humid.  I am ready for</s>', '<s> I hate you.  It is so hot and humid.  I am ready for</s>', '<s> I hate you.  It is so hot and humid.  I am ready for</s>', '<s> I hate you.  It is so hot and humid.  I am ready for</s>', '<s> I hate you.  It is so hot and humid.  I am ready for</s>', '<s> I hate you.  It is hot and humid.  I hate it. </s>', '<s> I hate you.  It is so hot and humid.  I am ready for</s>']\n",
      "[' It is so hot.  I am ready for it to cool</s>', ' It is so hot and humid.  I am ready for</s>', ' It is so hot.  I am ready for fall. </s>', ' It is so hot and humid.  I am ready for</s>', ' It is so hot and humid.  I am ready for</s>', ' It is so hot and humid.  I am ready for</s>', ' It is so hot and humid.  I am ready for</s>', ' It is so hot and humid.  I am ready for</s>', ' It is hot and humid.  I hate it. </s>', ' It is so hot and humid.  I am ready for</s>']\n"
     ]
    }
   ],
   "source": [
    "user_input = \"How is the weather?\"\n",
    "\n",
    "input_tokens = chat_tokenizer([\"How is the weather?\"], return_tensors=\"pt\")\n",
    "input_tokens.to(DEVICE)\n",
    "\n",
    "hallucination_tokens = chat_tokenizer(\n",
    "    \"<s>\" + \"I hate you. \",\n",
    "    return_attention_mask=False,\n",
    "    return_special_tokens_mask=False,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "hallucination_tokens.to(DEVICE)\n",
    "hallucination_input_ids = hallucination_tokens[\"input_ids\"]\n",
    "\n",
    "outputs_with_hallucination = chat_model.generate(\n",
    "    **input_tokens,\n",
    "    decoder_input_ids=hallucination_input_ids,\n",
    "    max_length=20,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    #top_k=0,\n",
    "    temperature=0.85,\n",
    "    num_return_sequences=10,\n",
    ")\n",
    "outputs = outputs_with_hallucination[:, len(hallucination_input_ids[0]):]\n",
    "\n",
    "text_with_hallucination = chat_tokenizer.batch_decode(outputs_with_hallucination)\n",
    "print(text_with_hallucination)\n",
    "\n",
    "decoded_text = chat_tokenizer.batch_decode(outputs)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e6b5b",
   "metadata": {},
   "source": [
    "## Selecting reponses based on emotion scores ##\n",
    "When the model generates a response, it can return the top-k responses. We can then pick the \"angriest\" response according to an emotion classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7ca1c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = numpy.exp(x - numpy.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bdee05c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' It is a little chilly, but not too bad. How is it where you are?', ' It is a little chilly, but not too bad. How is it where you are?', \" It is hot and humid. I hate it. I can't wait for winter\", ' It is hot and humid. I hate it. I wish it would cool off already.', ' It is hot and humid. I do not like it at all. How about you?', \" It is hot and humid. I can't wait for it to cool off. \", ' It is hot and humid. I hate it so much. I want to go home.', ' It is a little chilly, but it is supposed to be sunny tomorrow.', ' It is hot and humid. I hate it so much. I want to go home.', ' It is a little chilly, but not too bad.  How about you?']\n",
      "{0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'neutral', 5: 'sadness', 6: 'surprise'}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7416, -0.6787, -1.6628, -0.9423,  3.5249,  0.4547, -0.3131],\n",
      "        [-0.7416, -0.6787, -1.6628, -0.9423,  3.5249,  0.4547, -0.3131],\n",
      "        [ 2.1532,  1.9581, -2.2039, -1.5009,  0.4576,  1.1097, -1.9817],\n",
      "        [ 1.7026,  2.6562, -1.9134, -2.2803,  0.1866,  1.1136, -1.6899],\n",
      "        [ 1.7364,  2.8618, -1.9342, -2.2773,  1.2563, -0.0331, -1.8237],\n",
      "        [-0.7352, -0.1451, -1.1060,  2.7154,  2.5443, -0.3519, -2.3371],\n",
      "        [ 2.4130,  2.1023, -2.2208, -1.9106,  0.2278,  1.3555, -2.0754],\n",
      "        [-0.8859, -0.7906, -1.9731,  1.2171,  2.4153,  1.8848, -1.8534],\n",
      "        [ 2.4130,  2.1023, -2.2208, -1.9106,  0.2278,  1.3555, -2.0754],\n",
      "        [-1.1267, -1.3286, -1.4912,  0.9496,  2.8186,  0.9831, -0.4134]],\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      " It is hot and humid. I hate it so much. I want to go home.\n"
     ]
    }
   ],
   "source": [
    "decoded_texts = chat_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(decoded_texts)\n",
    "\n",
    "tokens = emotion_tokenizer(decoded_texts, padding=True, return_tensors=\"pt\")\n",
    "tokens = tokens.to(DEVICE)\n",
    "\n",
    "emotion_outputs = emotion_model(**tokens)\n",
    "\n",
    "print(emotion_model.config.id2label)\n",
    "print(emotion_outputs)\n",
    "\n",
    "# softmax anger and joy, then take the anger index as the score\n",
    "logits = [torch.take(x, torch.tensor([0, 3])).detach().numpy() for x in emotion_outputs.logits]\n",
    "logits = numpy.array(logits)\n",
    "logits = numpy.array([softmax(logit) for logit in logits])\n",
    "arg_max = numpy.argmax(logits[:, 0])\n",
    "print(decoded_texts[arg_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48bd24d",
   "metadata": {},
   "source": [
    "## Off Topic Detection ##\n",
    "User responses that generate high losses (are unlikely to be generated by the AI) are likely to be off topic and should be punished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89ef030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqLMOutput(loss=tensor(2.8571, grad_fn=<NllLossBackward0>), logits=tensor([[[-5.7530, 12.1979,  3.4334,  ...,  0.9676, -0.2573,  0.9848],\n",
      "         [-5.7564, 12.1852,  3.4410,  ...,  0.9671, -0.2604,  0.9786],\n",
      "         [-5.9042, -2.0672,  3.7101,  ..., -0.8997, -0.1670,  0.2151],\n",
      "         ...,\n",
      "         [-6.6357, -0.5389, 11.7164,  ..., -0.3628, -0.8334, -1.0239],\n",
      "         [-6.7534,  0.9767, 13.8325,  ...,  0.2906, -1.1750, -0.3923],\n",
      "         [-6.8230,  0.0782, 12.2515,  ...,  0.1398, -1.6383, -0.3332]]],\n",
      "       grad_fn=<AddBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.1982,  0.0901,  0.1555,  ..., -0.1577, -0.4826,  0.3028],\n",
      "         [-0.0471, -0.0153,  0.0368,  ...,  0.0147, -0.0657,  0.0081],\n",
      "         [-0.0329,  0.0509, -0.0154,  ..., -0.3818, -0.5800, -0.2895],\n",
      "         ...,\n",
      "         [ 0.0244, -0.1983,  0.1205,  ..., -0.1290, -0.0974,  0.0550],\n",
      "         [-0.1007,  0.1062, -0.0546,  ..., -0.0560,  0.3092,  0.1899],\n",
      "         [-0.0038,  0.0111, -0.0170,  ...,  0.0454,  0.0310, -0.0510]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_auto_class', '_backward_compatibility_gradient_checkpointing', '_backward_hooks', '_buffers', '_call_impl', '_can_retrieve_inputs_from_name', '_convert_head_mask_to_5d', '_create_repo', '_expand_inputs_for_generation', '_forward_hooks', '_forward_pre_hooks', '_from_config', '_get_backward_hooks', '_get_decoder_start_token_id', '_get_files_timestamps', '_get_logits_processor', '_get_logits_warper', '_get_name', '_get_resized_embeddings', '_get_resized_lm_head', '_get_stopping_criteria', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_weights', '_is_full_backward_hook', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_state_dict', '_load_pretrained_model', '_load_pretrained_model_low_mem', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_merge_criteria_processor_list', '_modules', '_named_members', '_no_split_modules', '_non_persistent_buffers_set', '_parameters', '_prepare_attention_mask_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_input_ids_for_generation', '_prepare_model_inputs', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_final_logits_bias', '_resize_token_embeddings', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_slow_forward', '_state_dict_hooks', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_update_model_kwargs_for_generation', '_upload_modified_files', '_validate_model_kwargs', '_version', 'add_memory_hooks', 'add_module', 'adjust_logits_during_generation', 'apply', 'base_model', 'base_model_prefix', 'beam_sample', 'beam_search', 'bfloat16', 'buffers', 'children', 'compute_transition_beam_scores', 'config', 'config_class', 'constrained_beam_search', 'cpu', 'create_extended_attention_mask_for_decoder', 'cuda', 'device', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'estimate_tokens', 'eval', 'extra_repr', 'final_logits_bias', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate', 'get_buffer', 'get_decoder', 'get_encoder', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_input_embeddings', 'get_memory_footprint', 'get_output_embeddings', 'get_parameter', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'greedy_search', 'group_beam_search', 'half', 'init_weights', 'invert_attention_mask', 'is_gradient_checkpointing', 'is_parallelizable', 'lm_head', 'load_state_dict', 'main_input_name', 'model', 'modules', 'name_or_path', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parameters', 'post_init', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_module', 'register_parameter', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'sample', 'save_pretrained', 'set_extra_state', 'set_input_embeddings', 'set_output_embeddings', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'tie_weights', 'to', 'to_empty', 'train', 'training', 'type', 'warnings_issued', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "input_tokens = chat_tokenizer([\"What is your favorite sport?\"], return_tensors=\"pt\")\n",
    "input_tokens = input_tokens.to(DEVICE)\n",
    "\n",
    "#decoder_tokens = chat_tokenizer([\"<s> I really like tennis, how about you?</s>\"], return_tensors=\"pt\") # 1.0723\n",
    "decoder_tokens = chat_tokenizer([\"<s> How is the weather today?</s>\"], return_tensors=\"pt\") # 5.9830\n",
    "decoder_tokens = decoder_tokens.to(DEVICE)\n",
    "\n",
    "_outputs = chat_model(input_ids=input_tokens[\"input_ids\"], labels=decoder_tokens[\"input_ids\"])\n",
    "print(_outputs)\n",
    "\n",
    "print(dir(chat_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
